import h5py
from h5Reader import *
from pcCluster import *
import numpy as np
import open3d as o3d
import numpy as np
import cv2

# Constants setup before the program is ran - These are the camera intrinsics
# Includes the horizontale and verticle field of view in degrees
#     Gotten from code in the s3 bucket: occ-visualizer/icda_development_py/icda_lib/object_detection.py
HORIZONTAL_FOV_DEG = 108
VERTICAL_FOV_DEG = 81
IM_WIDTH = 320
IM_HEIGHT = 240

##########################################################################################################################
# Function for reading from the h5 file, and creating point clouds from the depth and color image
# Returns an array of the point clouds generated by the depth and color image
# Using open3d we can convert the depth and color image into an RGBD image and then into a point cloud - works alright
##########################################################################################################################
def readH5(path, filename):
  #kind of has a simulation of a clif/ a warehouse
  #Still works fairly fine with the origional simulated information
  f = h5py.File(path + filename, 'r')

  #Parsing the h5 file - we only want the front of the device for now
  frnt = f["robot_left_front_0"]
  frnt_frames = frnt["frames"]
  first_frms_list = frnt_frames.keys()

  pntClds = []

  #Function iterates through all the frames and converts them to pointclouds
  for key in first_frms_list:
    frm = frnt_frames[key]
    depth = frm['depth']
    base = frm['base_color']

    pcd = depthToPC(depth, base)
    pntClds.append(pcd)

  #viewPCs([pntClds])

  return pntClds

######################################################################################################
# Function that converts a depth and base image into a pointcloud
# Function takes in the depth and base image from the h5 file which is just an array of their values
# Function returns the point cloud that was converted from depth and base iamges
######################################################################################################
def depthToPC(depths, base):
  #First convert the images into numpy arrays to work with open3d
  #The base image contains values 0-255 so we divide them by 255 to convert them to the right format
  depths_arr = np.asarray(depths)
  base_arr = np.asarray(base)
  base_arr = np.divide(base_arr, 255)

  #print(base_arr.shape)
  #print(depths_arr)
  
  #Convert the base and depth arrays into open3d images so we can create an RGBDImage
  #b_Im = o3d.geometry.Image((base_arr.astype(np.float32)))
  #d_Im = o3d.geometry.Image((depths_arr.astype(np.float32)))
  #print(d_Im)
  #print(d_Im)
  
  #RGBDImage is created, as it allows us to then create a point cloud using open3d's class functions
  #rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(b_Im, d_Im, depth_scale=0.7, depth_trunc=500)

  #Here we create part of the camera intrinsics (horizontal and verticle field of view in pixel units)
  h_rad = HORIZONTAL_FOV_DEG/180.*np.pi
  v_rad = VERTICAL_FOV_DEG/180.*np.pi
  f_hor_pix= (0.5*IM_WIDTH)/np.tan(0.5*h_rad)
  f_vert_pix= (0.5*IM_HEIGHT)/np.tan(0.5*v_rad)

  camMtrx = np.asarray([[180.9,   0.,  162. ],
                        [  0.,  181.,  122. ],
                        [  0.,    0.,    1. ]])


  dist = [0.517, -0.274, -0.00012, -0.00014, 0.851]

  #dst = cv2.undistort(base_arr, camMtrx, dist, None, None)


  #Other half of the camera intrinsics - the width and height of the image, as well as the x, y principle point
  pic_width = 320
  pic_height = 240

  distCoeff = np.zeros((5, 1), np.float64)
  #distCoeff[0,0] = 0.517
  #distCoeff[1,0] = -0.274
  #distCoeff[2,0] = -0.00012
  #distCoeff[3,0] = -0.00014
  #distCoeff[4,0] = 0.851
  distCoeff[0,0] = 0.517
  distCoeff[1,0] = -0.620
  distCoeff[2,0] = 0.000
  distCoeff[3,0] = 0.000
  distCoeff[4,0] = 0.100

  cam = np.eye(3, dtype=np.float32)
  cam[0,2] = pic_width / 2.0
  cam[1,2] = pic_height / 2.0
  cam[0,0] = f_hor_pix
  cam[1,1] = f_vert_pix

  b_dst = np.asarray(cv2.undistort(base_arr, cam, distCoeff))
  d_dst = np.asarray(cv2.undistort(depths_arr, cam, distCoeff))

  #b_dst = base_arr
  #d_dst = depths_arr
  #print(dst)

  #cv2.imshow('dst', b_dst)
  #cv2.waitKey(0)
  #cv2.destroyAllWindows()
  b_Im = o3d.geometry.Image((b_dst.astype(np.float32)))
  d_Im = o3d.geometry.Image((d_dst.astype(np.float32)))
  rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(b_Im, d_Im, depth_scale=0.7, depth_trunc=70)

  #Convert the RGBDImage from the camera intrinsics provided 
  pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, o3d.camera.PinholeCameraIntrinsic(pic_width, pic_height, f_hor_pix, f_vert_pix, pic_width/2, pic_height/2))

  #Transform the pointcloud so that it is not upsidedown - gotten from the open3d tutorial on converting depth images to point clouds
  #source: http://www.open3d.org/docs/release/tutorial/geometry/rgbd_image.html
  pcd.transform([[1,0,0,0],[0,-1,0,0],[0,0,-1,0],[0,0,0,1]])

  return pcd


#Main function which reads the point clouds, 
    #clusters them and grabs the corresponding labels,
    #generates the bounding boxes, 
    #then overlays the bounding boxes ontop of the point cloud
if __name__=="__main__":
  pntClds = readH5("./", "actor_robot.h5")
  pcld_cluster, labels = clusterFilter(30, pntClds[0], v=True)
  bboxs = objBoundingBoxes(pcld_cluster, labels)

  bboxs.append(pntClds[0])

  o3d.visualization.draw_geometries(bboxs)

  #viewPCs([pntClds])

  pass
